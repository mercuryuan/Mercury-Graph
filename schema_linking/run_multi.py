import json
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import multiprocessing
from datetime import datetime

import config
from schema_enricher.utils.fk_filler import FKFiller
from schema_linking.sl1_ds import TableSelector
from schema_linking.sl2_ds import SubgraphSelector
from schema_linking.sl3 import CandidateSelector
from utils.dataloader import DataLoader
from utils.graphloader import GraphLoader
from schema_linking.surfing_in_graph import SchemaGenerator
from schema_linking.validator import SLValidator


class SchemaLinkingPipeline:
    def __init__(self, dataset_name, db_name, question_data):
        self.dataset_name = dataset_name
        self.db_name = db_name
        # é—®é¢˜æ ·ä¾‹æ•°æ®
        self.question_data = question_data
        self.question = question_data['question']
        self.question_id = question_data['question_id']
        if self.dataset_name == "bird":
            self.evidence = question_data.get('evidence', '')
            self.difficulty = question_data.get('difficulty', '')
        self.validator = SLValidator(dataset_name, db_name)
        self.sg = SchemaGenerator()
        self.sl1 = TableSelector(dataset_name, db_name, self.question_data)
        self.sl2 = SubgraphSelector(dataset_name, db_name, self.question_data)
        self.sl3 = CandidateSelector(dataset_name, db_name, self.question_data)
        self.sl4 = FKFiller(dataset_name, db_name)
        self.logs = []
        self.hint = ''
        self.reasoning = []
        self.per_table_results = {}  # ç”¨äºä¿å­˜æ¯ä¸ªèµ·ç‚¹çš„æœ€ç»ˆå­å›¾é€‰æ‹©ç»“æœ
        self.ultimate_answer = {}
        self.sl2_per_iterations = []
        self.sl3_iteration = 0
        self.default_logger = self._default_logger

    def _default_logger(self, msg):
        print(msg)
        self.logs.append(msg)

    def log(self, msg, logger=None):
        if logger:
            logger(msg)
        else:
            self.default_logger(msg)

    def save_log(self):
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        markdown_filename = os.path.join(config.SCHEMA_LINKING, "logs", self.dataset_name, self.db_name,
                                         f"{self.question_id} {timestamp}.md")
        os.makedirs(os.path.dirname(markdown_filename), exist_ok=True)
        with open(markdown_filename, "w", encoding="utf-8") as f:
            f.write("\n".join(self.logs))

    def save_final_results(self):
        # ç”Ÿæˆæ–‡ä»¶å: æ•°æ®é›†/æ•°æ®åº“åç›®å½•ä¸‹çš„ JSON æ–‡ä»¶
        filename = os.path.join(config.SCHEMA_LINKING, "results", self.dataset_name, f"{self.db_name}.json")
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™åŠ è½½å·²æœ‰ç»“æœï¼Œå¦åˆ™åˆå§‹åŒ–ä¸ºç©ºå­—å…¸
        if os.path.exists(filename):
            with open(filename, "r", encoding="utf-8") as f:
                existing_results = json.load(f)
        else:
            existing_results = {"results": []}

        # å½“å‰é—®é¢˜çš„ç»“æœ
        current_result = {
            "question_id": self.question_id,
            "question": self.question,
            "reasoning": self.reasoning,
            "schema_linking_result": self.per_table_results,
            "sl_iterations": {"sl2": self.sl2_per_iterations, "sl3": self.sl3_iteration},
            "final_results": self.ultimate_answer
        }
        # å°†å½“å‰ç»“æœè¿½åŠ åˆ°å·²æœ‰ç»“æœä¸­
        existing_results["results"].append(current_result)

        # ç¡®ä¿è·¯å¾„å­˜åœ¨åå†™å…¥æ–‡ä»¶
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(existing_results, f, indent=2, ensure_ascii=False)
        self.log(f"\næœ€ç»ˆç»“æœå·²ä¿å­˜è‡³ {filename}", )

    def save_sl_to_pipeline(self):
        """
        å°† SchemaLinkingPipeline çš„ç»“æœä¿å­˜åˆ° å®Œæ•´æµç¨‹çš„ç»“æœ ä¸­ï¼ˆä¸€ä¸ªé—®é¢˜ä¸€ä¸ªç»“æœï¼‰
        """
        filename = os.path.join(config.PROJECT_ROOT, "Results", f"{self.dataset_name}.json")

        # è¯»å–å·²æœ‰ç»“æœ
        if os.path.exists(filename):
            with open(filename, "r", encoding="utf-8") as f:
                existing_results = json.load(f)
        else:
            existing_results = {}

        # å¦‚æœå½“å‰æ•°æ®åº“è¿˜æ²¡æœ‰è®°å½•ï¼Œåˆ™åˆå§‹åŒ–
        if self.db_name not in existing_results:
            existing_results[self.db_name] = {}

        # å½“å‰é—®é¢˜çš„ç»“æœï¼ˆè¦†ç›–å†™å…¥ï¼‰
        existing_results[self.db_name][str(self.question_id)] = {
            "question": self.question,
            "reasoning": self.reasoning,
            "sl_iterations": {"sl2": self.sl2_per_iterations, "sl3": self.sl3_iteration},
            "schema_linking_results": self.ultimate_answer
        }

        # ç¡®ä¿è·¯å¾„å­˜åœ¨åå†™å…¥æ–‡ä»¶
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(existing_results, f, indent=2, ensure_ascii=False)

        self.log(f"\nå”¯ä¸€ç»“æœå·²ä¿å­˜è‡³ {filename}")

    def is_solved(self):
        """
        æ£€æŸ¥æŒ‡å®šé—®é¢˜ ID æ˜¯å¦å·²ç»è§£å†³
        """
        filename = os.path.join(config.PROJECT_ROOT, "Results", f"{self.dataset_name}.json")
        if os.path.exists(filename):
            with open(filename, "r", encoding="utf-8") as f:
                existing_results = json.load(f)
            if self.db_name in existing_results and str(self.question_id) in existing_results[self.db_name]:
                if existing_results[self.db_name][str(self.question_id)]["schema_linking_results"][
                    "to_solve_the_question"]["is_solvable"]:
                    self.log(f"é—®é¢˜ {self.question} å·²è§£å†³ï¼Œæ— éœ€é‡æ–°æ‰§è¡Œã€‚")
                    return True
        return False

    def select_initial_tables(self, question):
        db_schema = "\n".join(self.sg.generate_combined_description(table) for table in self.sg.tables)
        self.log("## ç¬¬ä¸€è½®è¡¨é€‰æ‹©")
        sl1_result = self.sl1.select_relevant_tables(db_schema, question)
        selected_tables = sl1_result.get("selected_entity", [])
        if not selected_tables:
            self.log("æœªæ‰¾åˆ°åˆé€‚çš„è¡¨ï¼Œç»ˆæ­¢æ‰§è¡Œã€‚")
        else:
            self.log(f"é€‰ä¸­çš„è¡¨: {selected_tables}")
        self.log("é€‰æ‹©ç†ç”±:")
        self.log("```json\n" + json.dumps(sl1_result['reasoning'], indent=2, ensure_ascii=False) + "\n```")
        self.log("é—®é¢˜åˆ†è§£:")
        self.log("```json\n" + json.dumps(sl1_result['the steps of decomposed the question'], indent=2,
                                          ensure_ascii=False) + "\n```")
        return selected_tables, sl1_result

    def expand_subgraph_once(self, question, selected_table, reasoning_json, logger=None):
        self.log(f"\n### èµ·å§‹è¡¨: {selected_table}", logger)
        self.log("#### ç¬¬ä¸€æ¬¡å­å›¾æ‰©å±•", logger)
        sl2_schema = self.sl2.generate_schema_description([selected_table])
        self.hint = self.sl2.generate_hint(reasoning_json)
        sl2_result = self.sl2.select_relevant_tables(sl2_schema, question, [selected_table], hint=self.hint)
        # éªŒè¯ã€ä¿®æ­£ç»“æœ
        if self.validator.validate_entities(
                sl2_result["selected_columns"]) and self.validator.validate_foreign_keys(
            sl2_result["selected_reference_path"]):
            sl2_result = self.validator.validate_and_correct(sl2_result)
            self.log("è¿­ä»£æ‰©å±•ç»“æœ:", logger)
        else:
            sl2_result = self.validator.validate_and_correct(sl2_result)
            self.log("ä¿®æ­£åçš„ç»“æœ:", logger)
        self.log("```json\n" + json.dumps(sl2_result, indent=2, ensure_ascii=False) + "\n```", logger)

        return sl2_result

    def iterate_until_solvable(self, question, sl2_result, max_iterations=10, logger=None):
        # è¿­ä»£ç›´åˆ°è¾¾åˆ°æœ€å¤§è½®æ•°æˆ–ç”Ÿæˆå¯è§£ææ–¹æ¡ˆï¼Œè¿”å›æœ€ç»ˆçš„ sl2_result
        is_solvable = sl2_result["to_solve_the_question"]["is_solvable"]
        self.log(f"åˆå§‹ is_solvable: {is_solvable}\n", logger)
        result_from_last_round = self.sl2.generate_result_from_last_round(json.dumps(sl2_result, indent=2))
        iteration = 0
        prev_selected_tables = list(sl2_result["selected_columns"].keys())
        sl2_schema = self.sl2.generate_schema_description(prev_selected_tables)

        while not is_solvable and iteration < max_iterations:
            iteration += 1
            self.log(f"\n#### ç¬¬ {iteration} è½®è¿­ä»£", logger)

            if not sl2_result.get("selected_columns"):
                self.log("selected_columns ä¸ºç©ºï¼Œç»ˆæ­¢å¾ªç¯ã€‚", logger)
                raise ValueError("selected_columns ä¸ºç©ºï¼Œæ— æ³•ç»§ç»­è¿­ä»£ã€‚")

            selected_tables = list(sl2_result["selected_columns"].keys())
            self.log(f"æ–°é€‰æ‹©çš„è¡¨:   \n{selected_tables}  \n", logger)

            # å¦‚æœå­å›¾è¿é€šï¼Œåˆ™é‡æ–°ç”Ÿæˆ schemaï¼›å¦åˆ™ä½¿ç”¨ä¸Šä¸€è½®çš„ schema
            if self.sg.explorer.is_subgraph_connected(selected_tables):
                sl2_schema = self.sl2.generate_schema_description(selected_tables)
            else:
                self.log("å­å›¾ä¸è¿é€šï¼Œæ²¿ç”¨ä¸Šä¸€è½®çš„ schemaã€‚", logger)

            # æ–°ä¸€è½® schema linking
            sl2_result = self.sl2.select_relevant_tables(
                sl2_schema, question, selected_tables, result_from_last_round, self.hint
            )

            # éªŒè¯å¹¶ä¿®æ­£ç»“æœ
            if self.validator.validate_entities(sl2_result["selected_columns"]) and \
                    self.validator.validate_foreign_keys(sl2_result["selected_reference_path"]):
                sl2_result = self.validator.validate_and_correct(sl2_result)
                self.log("è¿­ä»£æ‰©å±•ç»“æœ:", logger)
            else:
                self.log("åŸç»“æœ", logger)
                self.log("```json\n" + json.dumps(sl2_result, indent=2, ensure_ascii=False) + "\n```", logger)
                self.log("ä¿®æ­£åçš„ç»“æœ:", logger)
                sl2_result = self.validator.validate_and_correct(sl2_result)

            self.log("```json\n" + json.dumps(sl2_result, indent=2, ensure_ascii=False) + "\n```", logger)

            is_solvable = sl2_result["to_solve_the_question"]["is_solvable"]
            self.log(f"å½“å‰ is_solvable: {is_solvable}")
            result_from_last_round = self.sl2.generate_result_from_last_round(json.dumps(sl2_result, indent=2))

        return sl2_result, iteration

    def select_candidate(self, question, recommend_tables):
        self.log("## å€™é€‰æŸ¥è¯¢ç”Ÿæˆ")

        candidates = self.per_table_results
        final_selected_result, is_consistent = self.sl3.select_candidate(candidates, question, recommend_tables)
        final_selected_result = self.validator.validate_and_correct(final_selected_result)

        is_solvable = final_selected_result.get("to_solve_the_question", {}).get("is_solvable", False)

        # å¦‚æœä¸å¯è§£æï¼Œæœ€å¤šè¿­ä»£ 3 æ¬¡é‡æ–°é€‰æ‹©å€™é€‰
        result_from_last_round = None
        iteration = 0
        while not is_solvable and iteration < 3:
            iteration += 1
            self.log(f"\n#### å€™é€‰é€‰æ‹©ç¬¬ {iteration} æ¬¡é‡è¯•")
            if result_from_last_round:
                final_selected_result, is_consistent = self.sl3.select_candidate(candidates, question, recommend_tables,
                                                                                 result_from_last_round)
            else:
                final_selected_result, is_consistent = self.sl3.select_candidate(candidates, question, recommend_tables,
                                                                                 )
            # ç¡®ä¿å®ä½“æ­£ç¡®
            final_selected_result = self.validator.validate_and_correct(final_selected_result)
            is_solvable = final_selected_result.get("to_solve_the_question", {}).get("is_solvable", False)
            # å¦‚æœéªŒè¯åå¤±è´¥ï¼Œåˆ™å°†å¤±è´¥ç»“æœä½œä¸ºä¸‹ä¸€è½®è¿­ä»£çš„è¾…åŠ©ä¿¡æ¯
            if not is_solvable:
                result_from_last_round = json.dumps(final_selected_result, indent=2, ensure_ascii=False)

        if not is_solvable:
            self.log("\nâš ï¸ æœ€ç»ˆå€™é€‰æ–¹æ¡ˆä¾ç„¶ä¸å¯è§£æï¼Œè¯·æ£€æŸ¥å€™é€‰ç”Ÿæˆæ¨¡å—æˆ–äººå·¥ä»‹å…¥ã€‚")
        # å¾—åˆ°sl3è¿­ä»£æ¬¡æ•°
        self.sl3_iteration = iteration

        return final_selected_result, is_consistent

    def run(self):
        # é˜²æ­¢è¢«api ban
        time.sleep(1)
        # å¦‚æœé—®é¢˜å·²ç»è§£å†³è¿‡ï¼Œåˆ™è·³è¿‡
        if self.is_solved():
            return
        self.log(f"# æ•°æ®é›†: {self.dataset_name}")
        self.log(f"# æ•°æ®åº“: {self.db_name}")
        self.log(f"# è‡ªç„¶è¯­è¨€é—®é¢˜:")
        self.log(f"`{self.question.strip()}`")
        # ç»Ÿè®¡åˆå§‹è”é€šåˆ†é‡
        components = self.sg.explorer.obtain_all_connected_components_in_database(
            f"{self.dataset_name}_{self.question_id}")
        if len(components) == 1:
            self.log("### åˆå§‹çŠ¶æ€ï¼šæ•°æ®åº“è¿é€š")
        else:
            self.log(f"### âš ï¸åˆå§‹çŠ¶æ€ï¼šæ•°æ®åº“ä¸­å­˜åœ¨ {len(components)} ä¸ªä¸è¿é€šçš„åˆ†é‡")
            components_str = '  \n'.join(['`' + str(component) + '`' for component in components])
            self.log(f"#### è”é€šåˆ†é‡ï¼š  \n{components_str}")

        # é˜¶æ®µä¸€ï¼šè·å–æ‰€æœ‰åˆå§‹é€‰ä¸­çš„è¡¨
        selected_tables, sl1_result = self.select_initial_tables(self.question)
        if not selected_tables:
            self.save_log()
            return

        reasoning_json = json.dumps(sl1_result['reasoning'], indent=2, ensure_ascii=False)
        self.reasoning = reasoning_json

        # é˜¶æ®µäºŒ & ä¸‰ï¼šå¹¶å‘å¤„ç†æ¯ä¸ªèµ·ç‚¹è¡¨
        def process_start_table(table):
            local_log = []

            def local_logger(msg):
                local_log.append(msg)

            self.log(f"\n---\n## èµ·ç‚¹è¡¨ `{table}` çš„æ‰©å±•ä¸æ¨ç†è¿‡ç¨‹", logger=local_logger)

            # æ‰§è¡Œé¦–æ¬¡æ‰©å±•
            sl2_result = self.expand_subgraph_once(self.question, table, reasoning_json, logger=local_logger)
            final_sl2_result, iteration = self.iterate_until_solvable(self.question, sl2_result, logger=local_logger)

            self.sl2_per_iterations.append({table: iteration})
            self.per_table_results[table] = final_sl2_result

            is_solvable = final_sl2_result.get("to_solve_the_question", {}).get("is_solvable", False)
            if is_solvable:
                local_log.append(f"\nâœ… èµ·ç‚¹ `{table}` æˆåŠŸç”Ÿæˆå¯è§£æçš„ SQL æŸ¥è¯¢æ–¹æ¡ˆã€‚")
            else:
                local_log.append(f"\nâŒ èµ·ç‚¹ `{table}` æœªèƒ½ç”Ÿæˆå¯è§£æ SQL æŸ¥è¯¢ã€‚")

            return "\n".join(local_log), is_solvable

        self.log("### âš™ï¸ å¹¶å‘æ‰§è¡Œæ¯ä¸ªèµ·ç‚¹è¡¨çš„æ¨ç†è¿‡ç¨‹\n")
        solvable_count = 0
        logs_per_table = []

        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_table = {executor.submit(process_start_table, table): table for table in selected_tables}
            for future in as_completed(future_to_table):
                table = future_to_table[future]
                try:
                    local_log, is_solvable = future.result()
                    logs_per_table.append((table, local_log))
                    if is_solvable:
                        solvable_count += 1
                except Exception as exc:
                    logs_per_table.append((table, f"\nâŒ èµ·ç‚¹ `{table}` æ‰§è¡Œå¤±è´¥: {exc}"))

        # ä¿è¯æ—¥å¿—æŒ‰åŸå§‹é¡ºåºè¾“å‡º
        for table in selected_tables:
            for t, log_text in logs_per_table:
                if t == table:
                    self.log(log_text)
                    break

        # æ€»ç»“
        if solvable_count > 0:
            self.log(f"\nğŸ¯ æœ‰æ•ˆæ–¹æ¡ˆï¼š{solvable_count}/{len(selected_tables)}ä¸ª")
        else:
            self.log("\nğŸ›‘ æ‰€æœ‰èµ·ç‚¹å‡æœªèƒ½æ‰¾åˆ°å¯è§£ææ–¹æ¡ˆï¼Œå»ºè®®äººå·¥æ£€æŸ¥ã€‚")
        # é˜¶æ®µå››ï¼šé€‰æ‹©å€™é€‰æŸ¥è¯¢
        self.ultimate_answer, is_consistent = self.select_candidate(self.question, reasoning_json)
        self.log("æ— éœ€ LLMä»‹å…¥ï¼š" + str(is_consistent))
        self.log("# æœ€ç»ˆé€‰æ‹©çš„å€™é€‰æŸ¥è¯¢:")
        self.log("```json\n" + json.dumps(self.ultimate_answer, indent=2, ensure_ascii=False) + "\n```")
        # ä¿å­˜æ—¥å¿—å’Œæœ€ç»ˆ JSON ç»“æœ
        self.save_log()
        self.save_final_results()
        # ä¿å­˜åˆ°å®Œæ•´æ€»æµç¨‹çš„ç»“æœä¸­
        self.save_sl_to_pipeline()


def run_bird_dev():
    # è¯»å– BIRD å¼€å‘æ•°æ®é›†
    bird_loader = DataLoader("bird_dev")
    bird_dev_list = bird_loader.list_dbname()
    all_samples = bird_loader.filter_data(show_count=True)

    for db_name in bird_dev_list:
        if db_name != "card_games":
            continue  # è·³è¿‡å…¶ä»–æ•°æ®åº“ï¼Œåªè·‘ card_games

        # åªå¯¹ card_games æ•°æ®åº“å¤„ç†
        db_samples = [sample for sample in all_samples if sample["db_id"] == db_name]

        # åŠ è½½å›¾ç»“æ„ï¼ˆä¸€æ¬¡å³å¯ï¼‰
        graph_loader = GraphLoader()
        graph_loader.load_graph("bird", db_name)

        for sample in db_samples:
            pipeline = SchemaLinkingPipeline("bird", db_name, sample)
            pipeline.run()
            # print(pipeline.question)
            # print(pipeline.question_id)
            break  # æµ‹è¯•ç”¨ï¼Œå…ˆåªè·‘ä¸€æ¡


def process_sample_bird_with_retry(args, max_retries=2):
    dataset_name, db_name, sample = args
    for attempt in range(max_retries + 1):
        try:
            pipeline = SchemaLinkingPipeline(dataset_name, db_name, sample)
            pipeline.run()
            return None  # è¡¨ç¤ºæˆåŠŸï¼Œæ— é”™è¯¯ä¿¡æ¯
        except Exception as e:
            if attempt == max_retries:
                return {
                    "db_name": db_name,
                    "question_id": sample.get("question_id"),
                    "question": sample.get("question"),
                    "error": str(e)
                }


def run_bird_dev_bx():
    # åŠ è½½ BIRD æ•°æ®
    bird_loader = DataLoader("bird_dev")
    bird_dev_list = bird_loader.list_dbname()
    all_samples = bird_loader.filter_data(show_count=True)

    # æ·»åŠ  question_id
    for question_id, sample in enumerate(all_samples):
        sample["question_id"] = question_id

    for db_name in bird_dev_list:
        # if db_name != "card_games":
        #     continue  # åªå¤„ç† card_games æ•°æ®åº“

        print(f"\n=== å¼€å§‹å¤„ç†æ•°æ®åº“: {db_name} ===")

        # åŠ è½½å›¾ç»“æ„
        graph_loader = GraphLoader()
        graph_loader.load_graph("bird", db_name)

        # å‡†å¤‡ä»»åŠ¡
        db_samples = [sample for sample in all_samples if sample["db_id"] == db_name]
        task_list = [("bird", db_name, sample) for sample in db_samples]

        failed_logs = []
        max_workers = min(20, multiprocessing.cpu_count())
        progress_bar = tqdm(total=len(task_list), desc=f"{db_name} è¿›åº¦", ncols=80)

        # æ‰§è¡Œçº¿ç¨‹æ±  + è‡ªåŠ¨é‡è¯•
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_task = {executor.submit(process_sample_bird_with_retry, task): task for task in task_list}

            for future in as_completed(future_to_task):
                result = future.result()
                if result is not None:
                    failed_logs.append(result)
                progress_bar.update(1)

        progress_bar.close()
        print(f"=== âœ… å®Œæˆæ•°æ®åº“: {db_name} ===")

        # æ¯ä¸ªæ•°æ®åº“å¤„ç†åä¿å­˜å¤±è´¥æ—¥å¿—ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        if failed_logs:
            file = os.path.join(config.PROJECT_ROOT, "Results", "bird.md")
            with open(file, "a", encoding="utf-8") as f:
                f.write(f"# âŒ æ•°æ®åº“ `{db_name}` å¤„ç†å¤±è´¥æ ·æœ¬è®°å½•\n\n")
                for item in failed_logs:
                    f.write(f"## é—®é¢˜ ID: {item['question_id']}\n")
                    f.write(f"**Question:** {item['question']}\n\n")
                    f.write(f"**Error:** `{item['error']}`\n\n---\n\n")
            print(f"âš ï¸ é”™è¯¯æ—¥å¿—å·²è¿½åŠ å†™å…¥ `{file}`\n")


def run_spider_dev():
    # è¯»å– spider å¼€å‘æ•°æ®é›†
    spider_loader = DataLoader("spider_dev")
    spider_dev_list = spider_loader.list_dbname()
    data = spider_loader.filter_data(fields=["db_id", "sql", "question"],
                                     show_count=True)
    # ç»™dataæ·»åŠ ä¸€ä¸ªquestion_idå­—æ®µ
    for question_id, d in enumerate(data):
        d['question_id'] = question_id

    for database in spider_dev_list:
        # if database != "tvshow":
        #     continue  # è·³è¿‡å…¶ä»–æ•°æ®åº“
        graph_loader = GraphLoader()
        graph_loader.load_graph("spider", database)
        database_data = [item for item in data if item["db_id"] == database]
        for sample in database_data:
            pipeline = SchemaLinkingPipeline("spider", database, sample)
            pipeline.run()


def process_sample(args):
    dataset_name, database, sample = args
    pipeline = SchemaLinkingPipeline(dataset_name, database, sample)
    pipeline.run()


def run_spider_dev_bx():
    # åŠ è½½æ•°æ®
    spider_loader = DataLoader("spider_dev")
    spider_dev_list = spider_loader.list_dbname()
    data = spider_loader.filter_data(fields=["db_id", "sql", "question"], show_count=True)

    for question_id, d in enumerate(data):
        d['question_id'] = question_id

    for database in spider_dev_list:
        print(f"\n=== å¼€å§‹å¤„ç†æ•°æ®åº“: {database} ===")

        # åŠ è½½å›¾ç»“æ„
        graph_loader = GraphLoader()
        graph_loader.load_graph("spider", database)

        # æå–è¯¥æ•°æ®åº“å¯¹åº”çš„æ ·æœ¬
        database_data = [item for item in data if item["db_id"] == database]
        task_list = [("spider", database, sample) for sample in database_data]

        # è®¾ç½®çº¿ç¨‹æ±  & è¿›åº¦æ¡
        max_workers = min(16, multiprocessing.cpu_count())
        progress_bar = tqdm(total=len(task_list), desc=f"{database} è¿›åº¦", ncols=80)

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_sample = {executor.submit(process_sample, task): task for task in task_list}

            for future in as_completed(future_to_sample):
                try:
                    future.result()
                except Exception as e:
                    print(f"âŒ æŸä¸ªæ ·æœ¬å¤„ç†å¤±è´¥: {e}")
                finally:
                    progress_bar.update(1)

        progress_bar.close()
        print(f"=== âœ… å®Œæˆæ•°æ®åº“: {database} ===\n")


if __name__ == "__main__":
    # run_bird_dev()
    # run_spider_dev()
    run_bird_dev_bx()
    # run_spider_dev_bx()

    # # å•ä¸ªé—®é¢˜
    # GraphLoader().load_graph("bird", "card_games")
    # pipeline = SchemaLinkingPipeline("bird", "card_games",
    #                                  {
    #                                      "question": """Please list the names of the cards in the set "Hauptset Zehnte Edition".""",
    #                                      "question_id": "10086"})
    # pipeline.run()
